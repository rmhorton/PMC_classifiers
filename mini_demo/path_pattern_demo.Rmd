---
title: "Section Heading Path Pattern Matching Demo"
author: "Bob"
date: "2024-04-22"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Using logistic regression classifier coefficients as a search vector

This document has two goals:

* Train logistic regression text classifiers 
  - Use semantic vector as features.
  - Mining labels from section headings
* Reframing model scoring as vector search
  - Showing that we get the same ranking by vector similarity to the coefficient vector as we do from scoring with the model.

Database search is a theme here in that sometimes mined labels might only apply to a small fraction of documents, so we need to search through a large database of examples to find enough cases to supply training and test sets.

```{r libraries, warning=FALSE, message=FALSE}
library(dplyr)
library(ggplot2)
library(glmnet)
library(doMC)
library(ROCR)

DATA_DIR <- 'demo_data' # "/Volumes/UBUNTU\ 22_0/data/R_medicine/"

PRELIMINARY_MODELS_FILE <- "preliminary_models.Rds"
CLEANER_DATA_RESULTS_FILE <- "cleaner_data_results.Rds"

# Functions
plot_multiple_roc_curves <- function(sft_list){
  # sft_list: key=target name, value=dataframe with scores and flags
  targets = names(sft_list)
  colors <- rainbow(length(targets), end=2/3, s=0.8, v=0.8)
  plot(c(0,1), c(0,1), type='n', 
       xlab="False Positive Rate", ylab="True Positive Rate",
       main="ROC Curves", asp=1)
  abline(0, 1, col='lightgray', lty=2)
  text_x <- 0.95
  text_y <- 0.5
  text_step <- 0.07
  text(text_x, text_y, "AUC", adj=1)
  for (i in seq_along(targets)){
    target <- targets[[i]]
    sft <- sft_list[[target]]
    pred <- prediction(sft[['scores']], sft[['flags']])
    perf <- performance(pred, "tpr", "fpr")
    lines(perf@x.values[[1]], perf@y.values[[1]], lwd=2, col=colors[[i]])
    
    auc <- performance(pred, 'auc')@y.values[[1]]
    text(text_x, text_y - text_step * i, 
         sprintf("%s: %0.3f", target, auc),
         col=colors[[i]], adj=1)
    
  }
}

# pgvector functions from https://github.com/pgvector/pgvector-r/blob/master/dbx/example.R
pgvector.serialize <- function(v) {
  # stopifnot(is.numeric(v))
  paste0("[", paste(v, collapse=","), "]")
}

pgvector.unserialize <- function(v) {
  lapply(strsplit(substring(v, 2, nchar(v) - 1), ","), as.numeric)
}
```



```{r train_models}

training_set <- bind_rows(
  readRDS(file.path(DATA_DIR, 'data_sample_block_01.Rds'))
  , readRDS(file.path(DATA_DIR, 'data_sample_block_02.Rds'))
  , readRDS(file.path(DATA_DIR, 'data_sample_block_03.Rds'))
  , readRDS(file.path(DATA_DIR, 'data_sample_block_04.Rds'))
)
# 'path_pattern_demo_training_set_round3.Rds'
test_set <- readRDS(file.path(DATA_DIR, 'data_sample_block_05.Rds'))
# 'path_pattern_demo_test_set_round3.Rds'

patterns <- list(
  TITLE = '^title$',
  METHODS = 'methods',  # includes "Materials and Methods"
  RESULTS = 'results',  # includes "Results and Discussion"
  CONCLUSION = '^conclusion', # Any section path that starts with "Conclusion"
  DISCUSSION = 'discussion'  # includes "Results and Discussion"
)

X_train <- training_set[['vector']] %>% pgvector.unserialize %>% do.call('rbind', .)
X_test <- test_set[['vector']] %>% pgvector.unserialize %>% do.call('rbind', .)


flag_pattern <- function(pattern, text)
  pattern %>% grepl(text, ignore.case=TRUE) %>% as.numeric


flags_train <- patterns %>% lapply(flag_pattern, training_set[['section_path']]) %>% data.frame
flags_test <- patterns %>% lapply(flag_pattern, test_set[['section_path']]) %>% data.frame

fit_model_for_pattern <- function(pattern_name)
  cv.glmnet(x=X_train, y=flags_train[[pattern_name]], family='binomial', alpha=0,
            type.measure='auc', nfolds=10, parallel=TRUE)

registerDoMC(cores = 10)


# if (file.exists(PRELIMINARY_MODELS_FILE)){
#   clf_list <- readRDS(PRELIMINARY_MODELS_FILE)
# } else {
  clf_list <- names(patterns) %>% setNames(nm=.) %>% lapply(fit_model_for_pattern)
#   saveRDS(clf_list, PRELIMINARY_MODELS_FILE)
# }


```

```{r model_scores}
# get scores as vector without dimnames
get_scores <- function(clf) predict(clf, newx=X_test)[,1] 

scores_test <- clf_list %>% lapply(get_scores) %>% as.data.frame

cor(scores_test)
```

```{r plot_roc_curves, fig.height=6, fig.width=6}

scores_and_flags_for_target <- flags_test %>% 
  names %>% setNames(nm=.) %>% 
  lapply(function(nm) data.frame(target=nm, scores=scores_test[[nm]], flags=flags_test[[nm]]))


plot_multiple_roc_curves(scores_and_flags_for_target)

```

If our pattern misses some examples (like "Results and Discussion" not being counted as "Results" or "Discussion", or if it is counted as both) how would that affect our estimated classifier performance?

```{r focus_on_cleaner_data, fig.height=6, fig.width=6}

train_and_evaluate_model_on_selected_pmids <- function(pattern_name){
  training_set2 <- cbind(training_set, flags_train) %>% group_by(pmid) %>% filter( sum( get(pattern_name) ) > 0 )
  test_set2 <- cbind(test_set, flags_test) %>% group_by(pmid) %>% filter( sum( get(pattern_name) ) > 0 )
  X_train2 <- training_set2[['vector']] %>% pgvector.unserialize %>% do.call('rbind', .)
  X_test2 <- test_set2[['vector']] %>% pgvector.unserialize %>% do.call('rbind', .)
  
  clf <- cv.glmnet(x=X_train2, y=training_set2[[pattern_name]], family='binomial', alpha=0,
                   type.measure='auc', nfolds=10, parallel=TRUE)
  scores <- predict(clf, newx=X_test2)[,1]
  flags <- test_set2[[pattern_name]]
  pred <- prediction(scores, flags)
  perf <- performance(pred, "tpr", "fpr")
  auc <- performance(pred, 'auc')@y.values[[1]]
  return( list(pattern_name=pattern_name, scores=scores, flags=flags, auc=auc, clf=clf, perf=perf, training_set_size=nrow(training_set2), test_set_size=nrow(test_set2)) )
}


# if (file.exists(CLEANER_DATA_RESULTS_FILE)){
#   results <- readRDS(CLEANER_DATA_RESULTS_FILE)
# } else {

  results <- names(patterns) %>% setNames(nm=.) %>% 
              lapply(train_and_evaluate_model_on_selected_pmids)
# results %>% saveRDS(CLEANER_DATA_RESULTS_FILE)
# }


plot_multiple_roc_curves(results)

```

# Extract classifier parameters into a table.

```{r extract_classifier_parameters_table}
squash <- function(x) 1/(1 + exp(-x))

get_parameter_row <- function(clf){
  vector_length <- function(v) sqrt(sum(v*v))
  
  par <- clf %>% coef %>% as.matrix %>% '['(,1)
  intercept <- par[[1]]
  beta <- par[-1]
  beta_length <- vector_length(beta)
  unit_beta_str <- (beta/beta_length) %>% pgvector.serialize
  list(intercept=intercept, beta_length=beta_length, unit_beta=unit_beta_str)
}

coef_df <- results  %>% 
  lapply(function(row) get_parameter_row(row$clf)) %>% 
  bind_rows %>% bind_cols(target=names(results), .)

coef_df %>% tibble
```
# We can find the best matches using vector search instead of prediction

## Ranking by prediction

```{r ranking_by_prediction}
M_coef <- coef_df[['unit_beta']] %>% pgvector.unserialize %>% do.call('cbind', .)

vector_similarity <- X_test %*% M_coef

clf_name <- 'TITLE'
keep_cols <- c('pmid', 'paragraph_number', 'section_path', 'text')

top_by_prediction <- test_set[order(scores_test[[clf_name]], decreasing=TRUE), keep_cols] %>% head
knitr::kable(top_by_prediction, caption="Top matches by prediction")

```

# Ranking by vector similarity

```{r ranking_by_vector_similarity}
similarity_col <- vector_similarity[,which(coef_df$target == clf_name)]
top_by_vector_similarity <- test_set[order(similarity_col, decreasing=TRUE), keep_cols] %>% head
knitr::kable(top_by_vector_similarity, caption="Top matches by vector similarity")

```
```{r similarity_vs_scores}
scores <- scores_test[[clf_name]]
plot(similarity_col, scores, main=clf_name)
```

To Do:

* Reproduce predicted probability using coefficients, intercept, scaling factor, and squashing function.
