---
title: "Training Classifiers on Section Heading Patterns"
author: "Bob"
date: "2024-04-22"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Using logistic regression classifier coefficients as a search vector

This document demonstrates two main points:

* Logistic regression text classifiers are useful and practical.
  - Semantic vectors make powerful predictive features features.
  - Labels mined from section headings can be used to identify a wide variety of topics.
* Model scoring can be framed as vector search.
  - We get the same ranking by vector similarity to the coefficient vector as we do from scoring with the model.

Database search is a theme here in that sometimes mined labels might only apply to a small fraction of documents, so we need to search through a large database of examples to find enough cases to supply training and test sets.

```{r libraries, warning=FALSE, message=FALSE}

library(dplyr)
library(ggplot2)
library(DT)
library(glmnet)
library(doMC)
library(ROCR)
library(openxlsx)

DATA_DIR <- 'demo_data'

PRELIMINARY_MODELS_FILE <- "mini_demo_models.Rds"

# pgvector functions from https://github.com/pgvector/pgvector-r/blob/master/dbx/example.R
pgvector.serialize <- function(v) {
  # stopifnot(is.numeric(v))
  paste0("[", paste(v, collapse=","), "]")
}

pgvector.unserialize <- function(v) {
  lapply(strsplit(substring(v, 2, nchar(v) - 1), ","), as.numeric)
}
```



```{r training_and_test_sets}

training_set <- bind_rows(
  readRDS(file.path(DATA_DIR, 'data_sample_block_01.Rds'))
  , readRDS(file.path(DATA_DIR, 'data_sample_block_02.Rds'))
  , readRDS(file.path(DATA_DIR, 'data_sample_block_03.Rds'))
)

test_set <-  bind_rows(
  readRDS(file.path(DATA_DIR, 'data_sample_block_04.Rds'))
  , readRDS(file.path(DATA_DIR, 'data_sample_block_05.Rds'))
)

training_set %>% head
```

```{r feature_matrixes}
X_train <- training_set[['vector']] %>% pgvector.unserialize %>% do.call('rbind', .)
X_test <- test_set[['vector']] %>% pgvector.unserialize %>% do.call('rbind', .)

print(dim(X_train))
print(dim(X_test))
```

```{r patterns}
patterns <- list(
  TITLE = '^title$',            # titles are always marked with the pseudo-heading 'Title'
  ABSTRACT = '^abstract$',      # excludes abstracts with subsections
  INTRODUCTION = '^(introduction|background)$', # introduction or background
  METHODS = '.*methods',        # includes "Materials and Methods"
  RESULTS = '^results$',        # excludes "Results and Discussion"
  DISCUSSION = '^discussion$',  # excludes "Results and Discussion"
  ADVERSE_EVENTS = '.*adverse events.*' # any section path with the term 'adverse event'
)
# If you change a pattern, you need to rename or delete the PRELIMINARY_MODELS_FILE.

flag_pattern <- function(pattern, text) pattern %>% grepl(text, ignore.case=TRUE) %>% as.numeric
# Here 'flag' is a verb; find the pattern and mark it with a Boolean flag.

flags_train <- patterns %>% lapply(flag_pattern, training_set[['section_path']]) %>% data.frame
flags_test <- patterns %>% lapply(flag_pattern, test_set[['section_path']]) %>% data.frame

cat("Positive cases in training set:\n")
flags_train %>% colSums %>% t %>% t %>% print

cat("Positive cases in test set:\n")
flags_test %>% colSums %>% t %>% t %>% print
```
```{r example_of_flag_matrix}

flags_train %>% head

```

```{r train_models}
fit_model_for_pattern <- function(pattern_name)
  cv.glmnet(x=X_train, y=flags_train[[pattern_name]], family='binomial', alpha=0,
            type.measure='auc', nfolds=10, parallel=TRUE)


if (file.exists(PRELIMINARY_MODELS_FILE)){
  clf_list <- readRDS(PRELIMINARY_MODELS_FILE)
} else {
  registerDoMC(cores = 10)
  clf_list <- names(patterns) %>% setNames(nm=.) %>% lapply(fit_model_for_pattern)
  saveRDS(clf_list, PRELIMINARY_MODELS_FILE)
}


```

```{r model_scores}
# Run the predict function with each classifier on the test set

predict_prob <- function(clf)
  predict(clf, newx=X_test, s='lambda.1se', type='response')

pred_prob_test <- clf_list %>% lapply(predict_prob) %>% lapply(as.numeric) %>% as.data.frame %>% tibble

cor(pred_prob_test) %>% round(3)
```

```{r plot_roc_curves, fig.height=6, fig.width=6}

scores_and_flags_for_target <- flags_test %>% 
  names %>% setNames(nm=.) %>% 
  lapply(function(nm) data.frame(target=nm, scores=pred_prob_test[[nm]], flags=flags_test[[nm]]))


plot_multiple_roc_curves <- function(sft_list){
  # sft_list: key=target name, value=dataframe with scores and flags
  targets = names(sft_list)
  colors <- rainbow(length(targets), end=2/3, s=0.8, v=0.8)
  plot(c(0,1), c(0,1), type='n', 
       xlab="False Positive Rate", ylab="True Positive Rate",
       main="ROC Curves", asp=1)
  abline(0, 1, col='lightgray', lty=2)
  text_x <- 0.95
  text_y <- 0.6
  text_step <- 0.07
  text(text_x, text_y, "AUC", adj=1)
  for (i in seq_along(targets)){
    target <- targets[[i]]
    sft <- sft_list[[target]]
    pred <- prediction(sft[['scores']], sft[['flags']])
    perf <- performance(pred, "tpr", "fpr")
    lines(perf@x.values[[1]], perf@y.values[[1]], lwd=2, col=colors[[i]])
    
    auc <- performance(pred, 'auc')@y.values[[1]]
    text(text_x, text_y - text_step * i, 
         sprintf("%s: %0.3f", target, auc),
         col=colors[[i]], adj=1)
    
  }
}


plot_multiple_roc_curves(scores_and_flags_for_target)

```

If our pattern misses some examples (like "Results and Discussion" not being counted as "Results" or "Discussion", or if it is counted as both) how would that affect our estimated classifier performance?



# Extract classifier parameters into a table.

```{r extract_classifier_parameters_table}
get_parameter_row <- function(clf){
  vector_length <- function(v) sqrt(sum(v*v))
  
  par <- clf %>% coef(s='lambda.1se') %>% as.matrix %>% '['(,1)
  intercept <- par[[1]]
  beta <- par[-1]
  beta_length <- vector_length(beta)
  beta_unit_str <- (beta/beta_length) %>% pgvector.serialize
  list(intercept=intercept, beta_scaling_factor=beta_length, beta_unit_vector=beta_unit_str)
}

coef_df <- clf_list %>% lapply(get_parameter_row) %>% 
  bind_rows %>% bind_cols(target=names(clf_list), .)

coef_df
```

# We can find the best matches using vector search instead of prediction

## Add scores to test set

Here we compute the inner products, the classifier predictions of class probabilities, and our own re-calculation of these probabilities based on the inner products.

```{r add_scores_to_test_set}
# Get a list of the names of the patterns we are trying to predict
pattern_names <- names(patterns)

# Collect model coefficient vectors into a matrix
M_coef <- coef_df[['beta_unit_vector']] %>% pgvector.unserialize %>% do.call('cbind', .)

# Compute inner product by matrix multiplication and format as a dataframe
ip_test_df <- X_test %*% M_coef %>% # inner_product for all models on all cases
  as.data.frame %>%
  setNames(nm=paste0(pattern_names, '_ip'))

# Re-compute predicted probabilities from model parameters and inner products
predict_fun <- function(target, ip_df){
  squash <- function(x) 1/(1 + exp(-x))
  ip_vec <- ip_df[[paste0(target, '_ip')]]
  model_params <- coef_df[coef_df$target == target,]
  squash(model_params[['intercept']] + model_params[['beta_scaling_factor']] * ip_vec)
}

pred_prob2 <- pattern_names %>% 
  lapply(predict_fun, ip_test_df) %>% 
  as.data.frame %>%
  tibble

scored_test_set <- bind_cols(
  test_set %>% subset(select = -c(bucket, vector)), 
  ip_test_df,
  pred_prob_test %>% setNames(nm=paste0(pattern_names, '_pred')),
  pred_prob2 %>% setNames(nm=paste0(pattern_names,  '_pred2'))
)

write.xlsx(scored_test_set, "scored_test_set.xlsx")
```


The predicted probabilities are the same whether we use the predict function or computer them ourselves. note that the `all.equal` function computes 'near equality', so it can handle minor differences in numerical precision.

```{r comparee_predictions}

compare_pred_vs_pred2 <- function(target){
  pred_col1 <- paste0(target, '_pred')
  pred_col2 <- paste0(target, '_pred2')
  all.equal(scored_test_set[[pred_col1]], scored_test_set[[pred_col2]])
}

pattern_names %>% setNames(nm=.) %>% lapply(compare_pred_vs_pred2)

```


```{r show_table}

score_col <- 'ADVERSE_EVENTS_ip'

keep_cols <- c('section_path', 'text', score_col)
scored_test_set[keep_cols] %>% 
  arrange( desc(!!sym(score_col)) ) %>%
  head(8) %>%
  (knitr::kable)

```

The inner product between the model coefficient vector and the semantic embedding vector always has a monotonic relationship to the predicted probability. This is why we can search for the examples most similar to our coefficient vector and find the ones that would have the highest score from the model, without running the predictions on all the cases.

```{r plot_score_vs_ip_by_target}

# Poor man's pivot_longer
get_target_df <- function(target){
  pred_col <- paste0(target, '_pred')
  ip_col <-  paste0(target, '_ip')
  df <- scored_test_set[c('pmid', 'paragraph_number', 'section_path', 'text', pred_col, ip_col)]
  names(df)[5:6] <- c('prediction_score', 'inner_product')
  df['target'] <- target
  df
}

scored_test_set_long <- names(patterns) %>% lapply(get_target_df) %>% bind_rows

scored_test_set_long %>% 
  ggplot(aes(x=prediction_score, y=inner_product, col=target)) + 
  geom_point()

```

Note that the predicted probabilities for ADVERSE_EVENTS are always very low (near 0), even though the examples with the highest scores do in fact appear to be about adverse events.
