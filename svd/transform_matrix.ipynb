{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import scipy as sp \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform matrix experiments on interpretability\n",
    "\n",
    "JMA 12 AUg 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# File dependencies:\n",
    "model_data = pd.read_parquet(\"../understanding_classifiers/model_data_top_1k_descriptors.parquet\").reset_index()\n",
    "model_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The coefficient vectors create the transform matrix from the semantic embedding space to the the interpretable concept space\n",
    "\n",
    "Each row is one coefficient vector, for one of the label-terms, e.g. MeSH terms, that occupy the semantic embedding space. \n",
    "\n",
    "Stacking coefficient vectors in a matrix forms a transform from the embedding space to the concept space, whose dimensions are labeled by the label-terms. z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the vector field and expand it to multiple rows. \n",
    "concept_transforms = np.vstack((model_data.beta_unit_vector).map(lambda x: np.asarray(x)))\n",
    "transform_names= model_data.name\n",
    "row_count, column_count = concept_transforms.shape\n",
    "f'output dimension: {row_count}, input dimension: {column_count}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both the numpy version and the scipy version return the same results. \n",
    "\n",
    "\n",
    "u,s, vh = la.svd(concept_transforms)\n",
    "u.shape, vh.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_df = pd.DataFrame(u).set_index(transform_names)\n",
    "u_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(s)\n",
    "plt.title('singular values magnitudes versus rank.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All rows and columns are unit length and rows and columns are orthogonal for both matricies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(u_pd.T.corr())\n",
    "plt.title('concept vector correlation heatmap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(np.cov(vh.T))\n",
    "plt.title('concept vector correlation transpose heatmap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check u for unit lengths along both axes.\n",
    "np.any(np.apply_along_axis(la.norm, 0, u) - 1.0 < 1e4), np.any(np.apply_along_axis(la.norm, 1, u) - 1.0 < 1e4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check v for unit lengths along both axes. \n",
    "np.any(np.apply_along_axis(la.norm, 0, vh) - 1.0 < 1e4), np.any(np.apply_along_axis(la.norm, 1, vh.T) - 1.0 < 1e4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To use the SVD we constrain the rank of the singular values by just taking the upper left corner of s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct the transform from its SVD components\n",
    "# Re\n",
    "s_reconstruction = np.zeros([row_count, column_count])\n",
    "s_reconstruction[:column_count, :column_count] = np.diag(s)\n",
    "sub_concept_transforms = u @ s_reconstruction @ vh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does the svd reconstruct the original matrix? \n",
    "# np.allclose := two arrays are element-wise equal within a tolerance\n",
    "np.allclose(concept_transforms, sub_concept_transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a limited rank approximation to the transform matrix matrix\n",
    "\n",
    "sub_rank = column_count - 600\n",
    "\n",
    "s_reconstruction = np.zeros([sub_rank, sub_rank])\n",
    "s_reconstruction[:sub_rank, :sub_rank] = np.diag(s[:sub_rank])\n",
    "sub_concept_transforms = u[:row_count, :sub_rank] @ s_reconstruction @ vh[:sub_rank,:column_count]\n",
    "sub_concept_transforms.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the normalized distance to the reduced rank matrix. \n",
    "# How good is the low rank approximation? \n",
    "la.norm(concept_transforms - sub_concept_transforms) /la.norm(concept_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transfer_sample_indexes = np.sort(np.random.choice(range(row_count), column_count, replace=False))\n",
    "hold_out_indexes = set(range(1000)).difference(set(transfer_sample_indexes))   # We loose the association with concept names sigh. \n",
    "# Check that the two sets are  mutually exclusive & exhaustive\n",
    "len(transfer_sample_indexes) + len(hold_out_indexes), hold_out_indexes.intersection(set(transfer_sample_indexes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distances are not preserved from embedding to concept space by the transform\n",
    "\n",
    "Make pair-wise comparisons between vectors in the hold out set and compare distances\n",
    "before transform and after transform.\n",
    "\n",
    "Look at the names of the hold out vectors, to see if distances between concept vectors\n",
    "are more meaningful than among embedding vectors. \n",
    "hold_out_names = transform_names[list(hold_out_indexes)]  # set ordering is not preserved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for pairs of held-out vectors, compute their cosine distances in embedding and concept spaces\n",
    "\n",
    "# normalized cosine distance'\n",
    "def cos_d(v1, v2):\n",
    "    return sp.spatial.distance.cosine(v1, v2) - 1\n",
    "\n",
    "transform = concept_transforms[transfer_sample_indexes,:]\n",
    "hold_out_vectors = concept_transforms[list(hold_out_indexes),:]\n",
    "    \n",
    "\n",
    "def before_after_distance(v1, v2, transform_matrix):\n",
    "    'compare how the transform changes the cosine distance'\n",
    "    before  = cos_d(v1, v2)\n",
    "    after = cos_d(transform_matrix @ v1, transform_matrix @ v2)\n",
    "    return before, after\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ba_distances = np.zeros([(len(hold_out_indexes)-1), 2])\n",
    "for k in range(len(hold_out_indexes)-1):\n",
    "    ba_distances[k, 0], ba_distances[k, 1] = before_after_distance(hold_out_vectors[k], hold_out_vectors[k+1], transform)\n",
    "    #print(f'before: {b:.3}, after: {a:.3}')\n",
    "\n",
    "ba_low_dim_distances = np.zeros([(len(hold_out_indexes)-1), 2])\n",
    "for k in range(len(hold_out_indexes)-1):\n",
    "    ba_low_dim_distances[k, 0], ba_low_dim_distances[k, 1] = before_after_distance(hold_out_vectors[k], hold_out_vectors[k+1], sub_concept_transforms)\n",
    "\n",
    "plt.plot(ba_distances[:,0], ba_distances[:,1], 'o')\n",
    "\n",
    "plt.plot(ba_low_dim_distances[:,0], ba_low_dim_distances[:,1], 'ro')\n",
    "plt.plot([-1, 1], [-1, 1], color= 'grey')\n",
    "plt.title('Transformed distances, Blue-full transform, Red-Low rank transform')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjecture - the concept vectors have lower entropy in the concept space. \n",
    "\n",
    "Create a hold-out set of coefficient vectors to test the transfer function.  Use them as test vectors in the embedding space.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as ss \n",
    "\n",
    "tranformed_hold_out_vectors = (transform @ hold_out_vectors.T).T\n",
    "\n",
    "# ss.entropy(concept_transforms + 1E-1 * np.ones([row_count, column_count]))\n",
    "x = np.abs(hold_out_vectors)\n",
    "xe = ss.entropy(x)/ss.entropy(np.ones([len(x)]))\n",
    "xe = np.sort(xe)\n",
    "print('mean: ',np.mean(xe))\n",
    "\n",
    "x1 = np.abs(tranformed_hold_out_vectors)\n",
    "xe1 = ss.entropy(x1)/ss.entropy(np.ones([len(x1)]))\n",
    "xe1 = np.sort(xe1)\n",
    "print('mean: ',np.mean(xe1))\n",
    "\n",
    "plt.plot(xe, xe1, 'og')\n",
    "plt.plot([0.92, 0.96], [0.92,0.96], color='grey')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
