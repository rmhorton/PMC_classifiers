Semantic embedding models use deep learning techniques to compute vector representations so that passages of text with similar meaning will be assigned similar vectors. Similarity between embedding vectors is widely used to search databases: the embedding for a query passage is used to search a database for similar passages. Here we present an approach to generate vectors representing general concepts in a semantic embedding space, constructed from the coefficients of linear classifiers. This representation reframes model scoring as vector search. We illustrate the concept vector idea by training a wide range of ML classifiers on biomedical literature in the US National Library of Medicine PubMed Central open-access dataset. Using this rich source of biomedical text and associated metadata we show two general approaches for training ML models on this data; predicting section heading patterns, and predicting indexing terms (MeSH). We present pragmatic interpretable screening criteria to evaluate biases in these models such that potential users can reason about their suitability in particular contexts.